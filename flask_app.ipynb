{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flask app.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP1BV0dBKGr336PVAVlsH5H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syedareehaquasar/SBSPS-Challenge-4482-Sentiment-Analysis-of-COVID-19-tweets/blob/master/flask_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsi2E_HtTCVu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "37cfaf80-2740-4773-ee4f-8d86b3d13ba1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsfilBI8UoDe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "5c28de73-b00c-4aee-a1b8-ad21e1e68cdf"
      },
      "source": [
        "!pip install -q transformers==2.1.1\n",
        "!pip install -q torch\n",
        "!pip install -q plotly\n",
        "!pip install -q flask\n",
        "!pip install tokenizers\n",
        "!pip install flask-socketio\n",
        "!pip install flask_restful"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 317kB 8.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 16.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 30.9MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/15/1c026f3aeafd26db30cb633d9915aae666a415179afa5943263e5dbd55a6/tokenizers-0.8.0-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 8.2MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.8.0\n",
            "Collecting flask-socketio\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/fa/ea1df958bd76a4a55b20dd87593839adf893e1fae0095b449fecdf325f21/Flask_SocketIO-4.3.1-py2.py3-none-any.whl\n",
            "Collecting python-socketio>=4.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/97/00741edd49788510b834b60a1a4d0afb2c4942770c11b8e0f6e914371718/python_socketio-4.6.0-py2.py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: Flask>=0.9 in /usr/local/lib/python3.6/dist-packages (from flask-socketio) (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from python-socketio>=4.3.0->flask-socketio) (1.12.0)\n",
            "Collecting python-engineio>=3.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/c8/793f17fe91343d70f76ad2bf6eac4e8c2d00c3c3ccb5173f18bc9938523e/python_engineio-3.13.1-py2.py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.9->flask-socketio) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.9->flask-socketio) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.9->flask-socketio) (2.11.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.9->flask-socketio) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.9->flask-socketio) (1.1.1)\n",
            "Installing collected packages: python-engineio, python-socketio, flask-socketio\n",
            "Successfully installed flask-socketio-4.3.1 python-engineio-3.13.1 python-socketio-4.6.0\n",
            "Collecting flask_restful\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/83/d0d33c971de2d38e54b0037136c8b8d20b9c83d308bc6c220a25162755fd/Flask_RESTful-0.3.8-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from flask_restful) (2018.9)\n",
            "Requirement already satisfied: six>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from flask_restful) (1.12.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask_restful) (1.1.2)\n",
            "Collecting aniso8601>=0.82\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/e4/787e104b58eadc1a710738d4e418d7e599e4e778e52cb8e5d5ef6ddd5833/aniso8601-8.0.0-py2.py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_restful) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_restful) (2.11.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_restful) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_restful) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask_restful) (1.1.1)\n",
            "Installing collected packages: aniso8601, flask-restful\n",
            "Successfully installed aniso8601-8.0.0 flask-restful-0.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY0TT7kuVBb3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "97f25e24-a103-46c8-9c2b-bdec0806c26d"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from pathlib import Path \n",
        "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import os\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "import random \n",
        "\n",
        "# fastai\n",
        "from fastai import *\n",
        "from fastai.text import *\n",
        "from fastai.callbacks import *\n",
        "\n",
        "# transformers\n",
        "from transformers import *\n",
        "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
        "\n",
        "# tensorflow\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "import tokenizers\n",
        "import pickle\n",
        "import math\n",
        "import re\n",
        "import string\n",
        "import seaborn as sns\n",
        "color = sns.color_palette()\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk import bigrams\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "eng_stopwords = stopwords.words('english')\n",
        "import collections\n",
        "from wordcloud import WordCloud\n",
        "from textwrap import wrap\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import networkx as nx\n",
        "\n",
        "from tweepy import OAuthHandler\n",
        "#from tweepy.streaming import StreamListener\n",
        "import tweepy\n",
        "import csv\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "from flask import Flask, request, jsonify, make_response\n",
        "from flask import render_template, url_for, flash, redirect\n",
        "from flask_socketio import SocketIO, emit\n",
        "from threading import Thread, Event\n",
        "\n",
        "from flask_restful import reqparse, abort, Api, Resource\n",
        "import json\n",
        "from flask import jsonify"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvAQDzxIVQwB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "app = Flask(__name__)\n",
        "app.config['SECRET_KEY'] = 'Cdzu1fRP1-4EjbmexesNFQ'\n",
        "socketio = SocketIO(app)\n",
        "\n",
        "# Twitter credentials\n",
        "consumer_key = 'eeSnutOqqknGGGqiso8DPEfdn'\n",
        "consumer_secret = 'uqKKBzpp96NJTBwEBge9wmsVKEJBdoSuMOmsaQiUphikReuJaH'\n",
        "access_key = \"1276097905363828736-RQfCc3FuSvhdwyYhjpoDfO7QH6Q4gB\"\n",
        "access_secret = \"nL8jdSU1j0cuRNiGGum8uTaLrq0blGEwqI8kmfwzAMSsJ\"\n",
        "\n",
        "# Pass your twitter credentials to tweepy via its OAuthHandler\n",
        "auth = OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_key, access_secret)\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve3w70rlW4P_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformersBaseTokenizer(BaseTokenizer):\n",
        "    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n",
        "    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', **kwargs):\n",
        "        self._pretrained_tokenizer = pretrained_tokenizer\n",
        "        self.max_seq_len = pretrained_tokenizer.max_len\n",
        "        self.model_type = model_type\n",
        "\n",
        "    def __call__(self, *args, **kwargs): \n",
        "        return self\n",
        "\n",
        "    def tokenizer(self, t:str) -> List[str]:\n",
        "        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\n",
        "        CLS = self._pretrained_tokenizer.cls_token\n",
        "        SEP = self._pretrained_tokenizer.sep_token\n",
        "        if self.model_type in ['roberta']:\n",
        "            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n",
        "            tokens = [CLS] + tokens + [SEP]\n",
        "        else:\n",
        "            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n",
        "            if self.model_type in ['xlnet']:\n",
        "                tokens = tokens + [SEP] +  [CLS]\n",
        "            else:\n",
        "                tokens = [CLS] + tokens + [SEP]\n",
        "        return tokens\n",
        "  \n",
        "class TransformersVocab(Vocab):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer):\n",
        "        super(TransformersVocab, self).__init__(itos = [])\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def numericalize(self, t:Collection[str]) -> List[int]:\n",
        "        \"Convert a list of tokens `t` to their ids.\"\n",
        "        return self.tokenizer.convert_tokens_to_ids(t)\n",
        "        #return self.tokenizer.encode(t)\n",
        "\n",
        "    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n",
        "        \"Convert a list of `nums` to their tokens.\"\n",
        "        nums = np.array(nums).tolist()\n",
        "        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)\n",
        "    \n",
        "    def __getstate__(self):\n",
        "        return {'itos':self.itos, 'tokenizer':self.tokenizer}\n",
        "\n",
        "    def __setstate__(self, state:dict):\n",
        "        self.itos = state['itos']\n",
        "        self.tokenizer = state['tokenizer']\n",
        "        self.stoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.itos)})\n",
        "\n",
        "# defining our model architecture \n",
        "class CustomTransformerModel(nn.Module):\n",
        "    def __init__(self, transformer_model: PreTrainedModel):\n",
        "        super(CustomTransformerModel,self).__init__()\n",
        "        self.transformer = transformer_model\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        \n",
        "        # attention_mask\n",
        "        # Mask to avoid performing attention on padding token indices.\n",
        "        # Mask values selected in ``[0, 1]``:\n",
        "        # ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n",
        "        attention_mask = (input_ids!=pad_idx).type(input_ids.type()) \n",
        "        \n",
        "        logits = self.transformer(input_ids,\n",
        "                                  attention_mask = attention_mask)[0]   \n",
        "        return logits\n",
        "\n",
        "def predict_sentiment(text):\n",
        "  sentiment = learner.predict(text)[1].item()\n",
        "  return sentiment\n",
        "\n",
        "def sentiment_label (Sentiment):\n",
        "   if Sentiment == 2:\n",
        "       return \"positive\"\n",
        "   elif Sentiment == 0:\n",
        "       return \"negative\"\n",
        "   else :\n",
        "       return \"neutral\"\n",
        "\n",
        "def replace_url(string): # cleaning of URL\n",
        "    text = re.sub(r'http\\S+', 'LINK', string)\n",
        "    return text\n",
        "\n",
        "\n",
        "def replace_email(text):#Cleaning of Email related text\n",
        "    line = re.sub(r'[\\w\\.-]+@[\\w\\.-]+','MAIL',str(text))\n",
        "    return \"\".join(line)\n",
        "\n",
        "def rep(text):#cleaning of non standard words\n",
        "    grp = text.group(0)\n",
        "    if len(grp) > 3:\n",
        "        return grp[0:2]\n",
        "    else:\n",
        "        return grp# can change the value here on repetition\n",
        "def unique_char(rep,sentence):\n",
        "    convert = re.sub(r'(\\w)\\1+', rep, sentence) \n",
        "    return convert\n",
        "\n",
        "def find_dollar(text):#Finding the dollar sign in the text\n",
        "    line=re.sub(r'\\$\\d+(?:\\.\\d+)?','PRICE',text)\n",
        "    return \"\".join(line)\n",
        "\n",
        "def replace_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "    u\"\\U0001F600-\\U0001F64F\" # emoticons\n",
        "    u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n",
        "    u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n",
        "    u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n",
        "    u\"\\U00002702-\\U000027B0\"\n",
        "    u\"\\U000024C2-\\U0001F251\"\n",
        "    \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'EMOJI', text) \n",
        "\n",
        "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']',\n",
        "          '>', '%', '=', '#', '*', '+', '\\\\', '•', '~', '@', '£', '·', '_', '{', '}', '©', '^',\n",
        "          '®', '`', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', 'Â', '█',\n",
        "          '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶',\n",
        "          '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼',\n",
        "          '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
        "          'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪',\n",
        "          '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√']\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    text = str(text)\n",
        "    for punct in puncts + list(string.punctuation):\n",
        "        if punct in text:\n",
        "            text = text.replace(punct, f'')\n",
        "    return text\n",
        "   \n",
        "def replace_asterisk(text):\n",
        "    text = re.sub(\"\\*\", 'ABUSE ', text)\n",
        "    return text\n",
        "\n",
        "def remove_duplicates(text):\n",
        "    text = re.sub(r'\\b(\\w+\\s*)\\1{1,}', '\\\\1', text)\n",
        "    return text\n",
        "\n",
        "def change(text):\n",
        "    if(text == ''):\n",
        "        return text\n",
        "  #calling the subfunctions in the cleaning function\n",
        "    text = replace_email(text)\n",
        "    text = replace_url(text)\n",
        "    text = unique_char(rep,text)\n",
        "    text = replace_asterisk(text)\n",
        "    text = remove_duplicates(text)\n",
        "    text = clean_text(text)\n",
        "    return text\n",
        "\n",
        "def extract_tweets(search_words,date_since, date_until, numTweets):\n",
        "  return(tweepy.Cursor(api.search, q=search_words, lang=\"en\", since=date_since, until=date_until, tweet_mode='extended').items(numTweets))\n",
        "\n",
        "def scraptweets(search_words, date_since, date_until, numTweets, numRuns):\n",
        "    # Define a pandas dataframe to store the date:\n",
        "    db_tweets = pd.DataFrame(columns = ['username', 'acctdesc', 'location', 'following', 'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts', 'retweetcount', 'text', 'hashtags'])\n",
        "    db_tweets['hashtags'] = db_tweets['hashtags'].astype('object')\n",
        "    #db_tweets = pd.DataFrame()\n",
        "\n",
        "    for i in range(numRuns):\n",
        "      tweets = extract_tweets(search_words,date_since,date_until,numTweets)\n",
        "      # Store these tweets into a python list\n",
        "      tweet_list = [tweet for tweet in tweets]\n",
        "      print(len(tweet_list))\n",
        "      noTweets = 0\n",
        "\n",
        "      for tweet in tweet_list:\n",
        "        username = tweet.user.screen_name\n",
        "        acctdesc = tweet.user.description\n",
        "        location = tweet.user.location\n",
        "        following = tweet.user.friends_count\n",
        "        followers = tweet.user.followers_count\n",
        "        totaltweets = tweet.user.statuses_count\n",
        "        usercreatedts = tweet.user.created_at\n",
        "        tweetcreatedts = tweet.created_at\n",
        "        retweetcount = tweet.retweet_count\n",
        "        hashtags = tweet.entities['hashtags']\n",
        "        lst=[]\n",
        "        for h in hashtags:\n",
        "          lst.append(h['text'])\n",
        "          try:\n",
        "              text = tweet.retweeted_status.full_text\n",
        "          except AttributeError:  # Not a Retweet\n",
        "              text = tweet.full_text\n",
        "\n",
        "          itweet = [username,acctdesc,location,following,followers,totaltweets,usercreatedts,tweetcreatedts,retweetcount,text,lst]\n",
        "          db_tweets.loc[len(db_tweets)] = itweet\n",
        "\n",
        "          noTweets += 1\n",
        "          print(noTweets)\n",
        "\n",
        "\n",
        "        print('no. of tweets scraped for run {} is {}'.format(i + 1, noTweets))\n",
        "        if i+1 != numRuns:\n",
        "            time.sleep(920)\n",
        "\n",
        "        filename = \"/content/analysis.csv\"\n",
        "        db_tweets['text'] = db_tweets['text'].apply(change)\n",
        "        db_tweets = db_tweets[['retweetcount', 'text']]\n",
        "        # Store dataframe in csv with creation date timestamp\n",
        "        db_tweets.drop_duplicates(subset =\"text\", keep = 'first', inplace = True)\n",
        "        db_tweets.to_csv(filename, index = False) #\n",
        "\n",
        "# Functions for Sentiment Extractor\n",
        "def save_weights(model, dst_fn):\n",
        "    weights = model.get_weights()\n",
        "    with open(dst_fn, 'wb') as f:\n",
        "        pickle.dump(weights, f)\n",
        "\n",
        "\n",
        "def load_weights(model, weight_fn):\n",
        "  with open(weight_fn, 'rb') as f:\n",
        "        weights = pickle.load(f)\n",
        "  model.set_weights(weights)\n",
        "  return model\n",
        "\n",
        "def loss_fn(y_true, y_pred):\n",
        "    # adjust the targets for sequence bucketing\n",
        "    ll = tf.shape(y_pred)[1]\n",
        "    y_true = y_true[:, :ll]\n",
        "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n",
        "        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n",
        "\n",
        "    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n",
        "    max_len = tf.reduce_max(lens)\n",
        "    ids_ = ids[:, :max_len]\n",
        "    att_ = att[:, :max_len]\n",
        "    tok_ = tok[:, :max_len]\n",
        "\n",
        "    onfig = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
        "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
        "    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n",
        "    \n",
        "    x1 = tf.keras.layers.Dropout(0.1)(x[0])\n",
        "    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
        "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    x1 = tf.keras.layers.Dense(1)(x1)\n",
        "    x1 = tf.keras.layers.Flatten()(x1)\n",
        "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "    \n",
        "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
        "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    x2 = tf.keras.layers.Dense(1)(x2)\n",
        "    x2 = tf.keras.layers.Flatten()(x2)\n",
        "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) \n",
        "    model.compile(loss=loss_fn, optimizer=optimizer)\n",
        "\n",
        "    # this is required as `model.predict` needs a fixed size!\n",
        "    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
        "    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
        "    \n",
        "    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n",
        "    return model, padded_model\n",
        "\n",
        "def generate_wordcloud(data,title):\n",
        "  wc = WordCloud(width=400, height=330, max_words=150,colormap=\"Dark2\",background_color='white', collocations=False).generate_from_frequencies(data)\n",
        "  plt.figure(figsize=(10,8))\n",
        "  plt.imshow(wc, interpolation='bilinear')\n",
        "  plt.tight_layout(pad=0)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GL2BeEllXtK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@app.route(\"/\",methods=['GET', 'POST'])\n",
        "@app.route(\"/home\", methods=['GET', 'POST'])\n",
        "def home():\n",
        "    return render_template('/content/dashboard.html')\n",
        "\n",
        "@app.route(\"/live_count\", methods=['GET', 'POST'])\n",
        "def live_count():\n",
        "    return render_template('/content/live_case_count.html')\n",
        "\n",
        "@app.route(\"/about\", methods=['GET', 'POST'])\n",
        "def about():\n",
        "    return render_template('/content/about_us.html')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNHVbPBsZ_G-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HOUR = 3600;\n",
        "\n",
        "thread = Thread()\n",
        "thread_stop_event = Event()\n",
        "\n",
        "class DailyUpdate():\n",
        "    def __init__(self):\n",
        "        self.delay = 24*HOUR\n",
        "        super(DailyUpdate, self).__init__()\n",
        "    def plotGenerator(self):\n",
        "        \"\"\"\n",
        "        Generates real time plots every 1 day.\n",
        "        \"\"\"\n",
        "        while not thread_stop_event.isSet():\n",
        "            # Initialise these variables:\n",
        "\n",
        "            search_words = \"(#India AND #COVID-19) OR #COVID19India\"\n",
        "            yesterday = datetime.datetime.now() - datetime.timedelta(days = 1)\n",
        "            date_since = yesterday.strftime(\"%Y-%m-%d\")\n",
        "            date_until = datetime.datetime.today().strftime('%Y-%m-%d')\n",
        "            numTweets = 2500\n",
        "            numRuns = 1\n",
        "            # Call the function scraptweets\n",
        "            program_start = time.time()\n",
        "            scraptweets(search_words, date_since, date_until, numTweets, numRuns)\n",
        "            program_end = time.time()\n",
        "\n",
        "            model_type = 'roberta'\n",
        "            pretrained_model_name = 'roberta-base'\n",
        "\n",
        "            model_class, tokenizer_class, config_class = RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
        "\n",
        "            transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\n",
        "            transformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\n",
        "            fastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])\n",
        "\n",
        "            pad_idx = transformer_tokenizer.pad_token_id\n",
        "\n",
        "            p = '/content/drive/My Drive/IBM_hack2020/model'\n",
        "            learner = load_learner(p, 'transformer.pkl')\n",
        "\n",
        "            path = '/content/analysis.csv'\n",
        "            predictions = pd.read_csv(path)\n",
        "\n",
        "            predictions['Prediction'] = predictions['text'].apply(predict_sentiment)\n",
        "            predictions['Prediction'] = predictions['Prediction'].apply(sentiment_label)\n",
        "            class_names = ['negative','positive','neutral']\n",
        "\n",
        "            predictions.rename(columns={'Prediction':'sentiment'},inplace=True)\n",
        "\n",
        "            MAX_LEN = 310\n",
        "            PAD_ID = 1\n",
        "            num_splits = 5\n",
        "            SEED = 88888\n",
        "\n",
        "            PATH = '/content/gdrive/My Drive/IBM_hack2020/Tf-Roberta/'\n",
        "            tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "                vocab_file=PATH+'vocab-roberta-base.json', \n",
        "                merges_file=PATH+'merges-roberta-base.txt', \n",
        "                lowercase=True,\n",
        "                add_prefix_space=True\n",
        "            )\n",
        "\n",
        "            test = predictions\n",
        "            test['len'] = test['text'].str.len()\n",
        "            test = test[test['len']<=310]\n",
        "            test.drop(\"len\",axis=1,inplace=True)\n",
        "            test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "            ct = test.shape[0]\n",
        "\n",
        "            input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
        "            attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "            token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "            sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
        "\n",
        "            for k in range(test.shape[0]):\n",
        "                    \n",
        "                # INPUT_IDS\n",
        "                text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
        "                enc = tokenizer.encode(text1)                \n",
        "                s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
        "                input_ids_t[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
        "                attention_mask_t[k,:len(enc.ids)+3] = 1\n",
        "\n",
        "            DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
        "            preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "            preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "\n",
        "            for fold in range(0,5):\n",
        "              \n",
        "              K.clear_session()\n",
        "              model, padded_model = build_model()\n",
        "              path = '/content/gdrive/My Drive/IBM_Hackathon_2020/R_CNN_weights/'\n",
        "              weight_fn = path+'v0-roberta-%i.h5'%(fold)\n",
        "\n",
        "              print('Loading model...')\n",
        "              # model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
        "              load_weights(model, weight_fn) \n",
        "              \n",
        "              print('Predicting Test...')\n",
        "\n",
        "              preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
        "              preds_start += preds[0]/num_splits\n",
        "              preds_end += preds[1]/num_splits\n",
        "\n",
        "            all = []\n",
        "            for k in range(input_ids_t.shape[0]):\n",
        "                a = np.argmax(preds_start[k,])\n",
        "                b = np.argmax(preds_end[k,])\n",
        "                if a>b: \n",
        "                    st = test.loc[k,'text']\n",
        "            \n",
        "            else:\n",
        "                    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
        "                    enc = tokenizer.encode(text1)\n",
        "                    st = tokenizer.decode(enc.ids[a-2:b-1])\n",
        "                all.append(st)\n",
        "            \n",
        "            test['selected_text'] = all\n",
        "            test.to_csv('/content/analysis.csv',index=False)\n",
        "\n",
        "            data=pd.read_csv(\"/content/analysis.csv\")\n",
        "            df = data.sentiment.value_counts()\n",
        "            size = list(df.values)\n",
        "            names = list(df.index)\n",
        "            fig = plt.figure(figsize=(10,10))\n",
        "            plt.xlabel(\"Sentiment\",Fontsize = 16)\n",
        "            plt.ylabel(\"Frequency\",Fontsize = 16)\n",
        "            sns.barplot(names,size,alpha = 0.8)\n",
        "            fig.savefig(\"/content/drive/My Drive/IBM_Hackathon_2020/Real-time-Data/Images/bar.png\")\n",
        "\n",
        "            df_new = pd.DataFrame(dict(\n",
        "                r=list(df.values),\n",
        "                theta=list(df.index)))\n",
        "            plt.figure(figsize=(10,10))\n",
        "            fig = px.line_polar(df_new, r='r', theta='theta', line_close=True)\n",
        "            fig.update_traces(fill='toself')\n",
        "            fig.write_image(\"/content/drive/My Drive/IBM_Hackathon_2020/Real-time-Data/Images/radar_plot.png\")\n",
        "\n",
        "            # Pie chart\n",
        "            labels = list(df.index)\n",
        "            sizes = list(df.values)\n",
        "            # only \"explode\" the 2nd slice \n",
        "            explode = (0.1, 0.1, 0.1)\n",
        "            #add colors\n",
        "            colors = ['#ff9999','#66b3ff','#99ff99']\n",
        "            fig1, ax1 = plt.subplots()\n",
        "            ax1.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',\n",
        "                    shadow=True, startangle=90)\n",
        "            # Equal aspect ratio ensures that pie is drawn as a circle\n",
        "            ax1.axis('equal')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('/content/drive/My Drive/IBM_Hackathon_2020/Real-time-Data/Images/pie_chart.png')\n",
        "\n",
        "            for i in range(3):\n",
        "              Data= data[data[\"sentiment\"]==df.index[i]]\n",
        "              Word_frequency = pd.Series(' '.join(Data.selected_text).split()).value_counts()[:20]#Calculating the words frequency\n",
        "              plt.figure(figsize=(25,10))\n",
        "              plt.ylabel(\"Frequency\",fontsize=16)\n",
        "              plt.title(\"Sentiment Triggers\")\n",
        "              sns.barplot(Word_frequency.index,Word_frequency.values,alpha=0.8)\n",
        "              plt.savefig(\"/content/drive/My Drive/IBM_Hackathon_2020/Real-time-Data/Images/wordfrequency_\"+df.index[i]+\".png\")\n",
        "\n",
        "            for i in range(0,2):\n",
        "                Analysis_Data = data\n",
        "                data[\"selected_text\"]=data[\"selected_text\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (eng_stopwords)]))\n",
        "                Sentiment = Analysis_Data[Analysis_Data['sentiment'] == df.index[i]]#Creating the dataframe of having same sentiment\n",
        "                Word_frequency = pd.Series(' '.join(Sentiment.selected_text).split()).value_counts()[:50]#Calculating the words frequency\n",
        "                generate_wordcloud(Word_frequency.sort_values(ascending=False),data.index[i])\n",
        "                plt.savefig(\"/content/drive/My Drive/IBM_Hackathon_2020/Real-time-Data/Images/Wordcloud_\" +df.index[i]+\" .png\")\n",
        "\n",
        "                data[\"text\"]=data[\"text\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (eng_stopwords)]))\n",
        "            bigrams = [b for l in data.text for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
        "            bigram_counts = collections.Counter(bigrams)\n",
        "            bigram_df = pd.DataFrame(bigram_counts.most_common(10),\n",
        "                                        columns=['bigram', 'frequency'])\n",
        "            x =bigram_df.bigram\n",
        "            y = bigram_df.frequency\n",
        "\n",
        "            fig, ax = plt.subplots(1, 1, figsize = (20, 15), dpi=300)\n",
        "            sns.barplot(x,y,alpha=0.8)\n",
        "            plt.ylabel(\"Frequency\",fontsize=16)\n",
        "            ax.set_xlabel('')\n",
        "            plt.savefig('/content/drive/My Drive/IBM_Hackathon_2020/Real-time-Data/Images/bigram_freq.png')\n",
        "\n",
        "            ext_data_negative = data[data[\"sentiment\"]=='negative']\n",
        "            ext_data_positive = data[data[\"sentiment\"]=='positive']\n",
        "\n",
        "            bigrams = [b for l in ext_data_positive.selected_text for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
        "            bigram_counts = collections.Counter(bigrams)\n",
        "            bigram_df_positive = pd.DataFrame(bigram_counts.most_common(60),\n",
        "                                        columns=['bigram', 'frequency'])\n",
        "            bigrams = [b for l in ext_data_negative.selected_text for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
        "            bigram_counts = collections.Counter(bigrams)\n",
        "            bigram_df_negative = pd.DataFrame(bigram_counts.most_common(80),\n",
        "                                        columns=['bigram', 'frequency'])\n",
        "\n",
        "            # Create network plot \n",
        "            G=nx.grid_2d_graph(2,2)\n",
        "\n",
        "            pos = nx.fruchterman_reingold_layout(G,k=10,iterations=100)\n",
        "            fig,ax = plt.subplots(figsize=(50,30)) \n",
        "            d = bigram_df_negative.set_index('bigram').T.to_dict('records')\n",
        "            for k, v in d[0].items():\n",
        "              G.add_edge(k[0], k[1], weight=(v * 10))\n",
        "            pos = nx.fruchterman_reingold_layout(G,k=10,iterations=100) \n",
        "              \n",
        "            nx.draw_networkx(G, pos,\n",
        "                            font_size=16,\n",
        "                            width=4,\n",
        "                            edge_color='#e25a4b',\n",
        "                            node_size=500,\n",
        "                            title = \"Negative Sentiment\",\n",
        "                            with_labels = False,\n",
        "                            ax=ax)\n",
        "            x_values, y_values = zip(*pos.values())\n",
        "            x_max = max(x_values)\n",
        "            x_min = min(x_values)\n",
        "            x_margin = (x_max - x_min) * 0.25\n",
        "            plt.xlim(x_min - x_margin, x_max + x_margin)\n",
        "\n",
        "            for key, value in pos.items():\n",
        "                x, y = value[0]+.135, value[1]+.045\n",
        "                ax.text(x, y,\n",
        "                        s=key,bbox=dict(facecolor='#ffcd94', alpha=0.4),\n",
        "                        horizontalalignment='center', fontsize=35)\n",
        "            plt.savefig(\"/content/drive/My Drive/IBM_Hackathon_2020/Real-time-Data/Images/ext_negative.png\")        \n",
        "            fig,ax  = plt.subplots(figsize=(50,30))\n",
        "            d = bigram_df_positive.set_index('bigram').T.to_dict('records')\n",
        "            for k, v in d[0].items():\n",
        "              G.add_edge(k[0], k[1], weight=(v * 10))\n",
        "            pos = nx.fruchterman_reingold_layout(G,k=10,iterations=100) \n",
        "              \n",
        "            nx.draw_networkx(G, pos,\n",
        "                            font_size=16,\n",
        "                            width=4,\n",
        "                            edge_color='#999894',\n",
        "                            node_size=500,\n",
        "                            with_labels = False,\n",
        "                            title = \"Positve Sentiment\",\n",
        "                            ax=ax)\n",
        "            x_values, y_values = zip(*pos.values())\n",
        "            x_max = max(x_values)\n",
        "            x_min = min(x_values)\n",
        "            x_margin = (x_max - x_min) * 0.25\n",
        "            plt.xlim(x_min - x_margin, x_max + x_margin)\n",
        "\n",
        "            # Create offset labels\n",
        "            for key, value in pos.items():\n",
        "                x, y = value[0]+.135, value[1]+.045\n",
        "                ax.text(x, y,\n",
        "                        s=key,bbox=dict(facecolor='#7c99d0', alpha=0.4),\n",
        "                        horizontalalignment='center', fontsize=35)\n",
        "            plt.savefig(\"/content/drive/My Drive/IBM_Hackathon_2020/Real-time-Data/Images/ext_positive.png\")\n",
        "\n",
        "            data[\"text\"]=data[\"text\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (eng_stopwords)]))\n",
        "            bigrams = [b for l in data.text for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
        "            bigram_counts = collections.Counter(bigrams)\n",
        "            bigram_df = pd.DataFrame(bigram_counts.most_common(60),\n",
        "                                        columns=['bigram', 'frequency'])\n",
        "            \n",
        "            d = bigram_df.set_index('bigram').T.to_dict('records')\n",
        "            # Create network plot \n",
        "            G = nx.Graph()\n",
        "            for k, v in d[0].items():\n",
        "                G.add_edge(k[0], k[1], weight=(v * 10))\n",
        "\n",
        "            fig,ax = plt.subplots(figsize=(20,20))\n",
        "            pos = nx.spring_layout(G,dim=2,k=5)\n",
        "\n",
        "            # Plot networks\n",
        "            nx.draw_networkx(G, pos,\n",
        "                            font_size=12,\n",
        "                            width=4,\n",
        "                            edge_color='grey',\n",
        "                            node_color='#4a4140',\n",
        "                            node_size=500,\n",
        "                            with_labels = False,\n",
        "                            ax=ax)\n",
        "            x_values, y_values = zip(*pos.values())\n",
        "            x_max = max(x_values)\n",
        "            x_min = min(x_values)\n",
        "            x_margin = (x_max - x_min) * 0.25\n",
        "            plt.xlim(x_min - x_margin, x_max + x_margin)\n",
        "\n",
        "\n",
        "            # Create offset labels\n",
        "            for key, value in pos.items():\n",
        "              x, y = value[0]+.135, value[1]+.045\n",
        "                ax.text(x, y,\n",
        "                        s=key,\n",
        "                        bbox=dict(facecolor='#ffcd94', alpha=0.4),\n",
        "                        horizontalalignment='center', fontsize=25)\n",
        "                \n",
        "            fig.savefig('/content/drive/My Drive/IBM_Hackathon_2020/Real-time-Data/Images/network.png')\n",
        "\n",
        "            fig = px.box(data, y=\"retweetcount\",points=\"all\")\n",
        "            fig.update_layout(\n",
        "                yaxis_title=\"Retweet Count\",\n",
        "                font=dict(\n",
        "                    family=\"Courier New, monospace\",\n",
        "                    size=18,\n",
        "                    color=\"#7f7f7f\"\n",
        "                )\n",
        "            )\n",
        "            fig.write_image('/content/drive/My Drive/IBM_Hackathon_2020/Real-time-Data/Images/retweet_count_boxplot.png')\n",
        "\n",
        "            fig = px.box(data, y=\"sentiment\",points=\"all\")\n",
        "            fig.update_layout(\n",
        "                yaxis_title=\"Sentiment\",\n",
        "                font=dict(\n",
        "                    family=\"Courier New, monospace\",\n",
        "                    size=18,\n",
        "                    color=\"#7f7f7f\"\n",
        "                )\n",
        "            )\n",
        "            fig.write_image('/content/drive/My Drive/IBM_Hackathon_2020/Real-time-Data/Images/sentiment_boxplot.png')\n",
        "\n",
        "            # socketio.emit('newnumber', {'number': number}, namespace='/test')\n",
        "            sleep(self.delay)\n",
        "    def run(self):\n",
        "        self.plotGenerator()\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QO075WYajZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@app.route('/realtime', methods=['GET', 'POST'])\n",
        "def realtime():\n",
        "    #only by sending this page first will the client be connected to the socketio instance\n",
        "    return render_template('/content/real_time_analysis.html')\n",
        "\n",
        "@socketio.on('connect', namespace='/analyze')\n",
        "def test_connect():\n",
        "    # need visibility of the global thread object\n",
        "    global thread\n",
        "    print('Client connected')\n",
        "    #Start the random number generator thread only if the thread has not been started before.\n",
        "    if not thread.isAlive():\n",
        "        print(\"Starting Thread\")\n",
        "        thread = DailyUpdate()\n",
        "        thread.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUzFWmqoantJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "api = Api(app)\n",
        "\n",
        "@app.route(\"/twitter\", methods=['GET', 'POST'])\n",
        "def twitter():\n",
        "    return render_template('/content/twitter_live_feed.html')\n",
        "\n",
        "class GetSenti(Resource):\n",
        "  def getsentiment(request):\n",
        "      data = {\"success\": False}\n",
        "      # if parameters are found, echo the msg parameter \n",
        "      if (request.data != None):  \n",
        "          with graph.as_default():\n",
        "              data[\"predictions\"] = predict(request.GET.get(\"text\"))\n",
        "          data[\"success\"] = True\n",
        "      return JsonResponse(data)\n",
        "\n",
        "api.add_resource(GetSenti, '/getsentiment', methods=['GET', 'POST'])\n",
        "\n",
        "class Analyze(Resource):\n",
        "  def analyzehashtag(request):\n",
        "      positive = 0\n",
        "      neutral = 0\n",
        "      negative = 0\n",
        "      \n",
        "      model_type = 'roberta'\n",
        "      pretrained_model_name = 'roberta-base'\n",
        "\n",
        "      model_class, tokenizer_class, config_class = RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
        "\n",
        "      transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\n",
        "      transformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\n",
        "      fastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])\n",
        "\n",
        "      pad_idx = transformer_tokenizer.pad_token_id\n",
        "\n",
        "      p = '/content/drive/My Drive/IBM_Hackathon_2020/Roberta_Model'\n",
        "      learner = load_learner(p, 'transformer.pkl')\n",
        "\n",
        "      for tweet in tweepy.Cursor(api.search,q=\"#\" + request.GET.get(\"text\") + \" -filter:retweets\",rpp=5,lang=\"en\", tweet_mode='extended').items(100):\n",
        "          with graph.as_default():\n",
        "              text = change(tweet.full_text)\n",
        "              prediction = predict_sentiment(text)\n",
        "              prediction = sentiment_label(prediction)\n",
        "          if(prediction == \"Positive\"):\n",
        "              positive += 1\n",
        "          if(prediction == \"Neutral\"):\n",
        "              neutral += 1\n",
        "          if(prediction == \"Negative\"):\n",
        "              negative += 1\n",
        "      return JsonResponse({\"positive\": positive, \"neutral\": neutral, \"negative\": negative});\n",
        "\n",
        "api.add_resource(Analyze, '/analyzehashtag', methods=['GET', 'POST'])\n",
        "\n",
        "class Gettweets(Resource):\n",
        "  def gettweets(request):\n",
        "      \n",
        "      model_type = 'roberta'\n",
        "      pretrained_model_name = 'roberta-base'\n",
        "\n",
        "      model_class, tokenizer_class, config_class = RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
        "\n",
        "      transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\n",
        "      transformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\n",
        "      fastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])\n",
        "\n",
        "      pad_idx = transformer_tokenizer.pad_token_id\n",
        "\n",
        "      p = '/content/drive/My Drive/IBM_hack2020/model'\n",
        "      learner = load_learner(p, 'transformer.pkl')\n",
        "\n",
        "      tweets = []\n",
        "\n",
        "      for tweet in tweepy.Cursor(api.search,q=\"#\" + request.GET.get(\"text\") + \" -filter:retweets\",rpp=5,lang=\"en\", tweet_mode='extended').items(50):\n",
        "          temp = {}\n",
        "          text = change(tweet.full_text)\n",
        "          temp[\"text\"] = text\n",
        "          temp[\"username\"] = tweet.user.screen_name\n",
        "          with graph.as_default():\n",
        "              text = change(tweet.full_text)\n",
        "              prediction = predict_sentiment(text)\n",
        "              prediction = sentiment_label(prediction)\n",
        "          temp[\"label\"] = prediction\n",
        "          tweets.append(temp)\n",
        "      return JsonResponse({\"results\": tweets});\n",
        "\n",
        "api.add_resource(Gettweets, '/gettweets', methods=['GET', 'POST'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O5IHLgpa5jv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0',port=5000, debug=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}